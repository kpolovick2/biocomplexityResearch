In a test of 120000 executions on a dataset of size n=10, the ILP solution required 0.0009174 seconds per execution on average.
The polynomial time algorithm required 9.331e-05 seconds per execution.
Overall, this was a 9.8318x speedup.
---------------------
In a test of 120 executions on a dataset of size n=1000, the ILP solution required 0.3777 seconds per execution on average.
The polynomial time algorithm required 0.2682 seconds per execution.
Overall, this was a 1.408x speedup.
This was likely drastically slowed due to the copying practices implemented in the algorithm. An in-place memory-efficient version of the algorithm will be tested in the future.
---------------------
In a test of 120 executions on a dataset of size n=1000, the ILP solution required 0.3726 seconds per execution on average.
The polynomial time algorithm required 0.2353 seconds per execution.
Overall, this was a 1.5838x speedup.
This speedup occurred because the code was modified to mutate the lists instead of copying them.
---------------------
When considering cases of n=25,200,and 1000, N=3, where the items are in two sets at first, then merged into one with a tag perturbation,
we find that the speedup of this new algorithm generally tends to increase as n increases, however in the diagonal case,
the speedup tends to decrease as n increases. In other words, the algorithm behaves like a polynomial-time
algorithm while the size of the descriptor is small
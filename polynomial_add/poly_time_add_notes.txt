To try:
- segment the lists into smaller lists, which are operated on in their entirety, then move on to other segments
- network flow algorithm
- min set cover algorithm
- try making a rust library to deal with memory more directly
---------------------
Tested:
- Cupy: drastically slower because hardware interfacing, could try again
- Storing indices instead of raw vectors: much faster, still slow
- Encoding as char: doesn't work in python because += not supported for char
---------------------
In a test of 120000 executions on a dataset of size n=10, the ILP solution required 0.0009174 seconds per execution on average.
The polynomial time algorithm required 9.331e-05 seconds per execution.
Overall, this was a 9.8318x speedup.
---------------------
In a test of 120 executions on a dataset of size n=1000, the ILP solution required 0.3777 seconds per execution on average.
The polynomial time algorithm required 0.2682 seconds per execution.
Overall, this was a 1.408x speedup.
This was likely drastically slowed due to the copying practices implemented in the algorithm. An in-place memory-efficient version of the algorithm will be tested in the future.
---------------------
In a test of 120 executions on a dataset of size n=1000, the ILP solution required 0.3726 seconds per execution on average.
The polynomial time algorithm required 0.2353 seconds per execution.
Overall, this was a 1.5838x speedup.
This speedup occurred because the code was modified to mutate the lists instead of copying them.
---------------------
In a test of 12 executions on a dataset of size n=1000, the ILP solution required 0.0338 seconds per execution.
THe polynomial time algorithm required 0.0853 seconds per execution.
Overall, this was a 0.03967x speedup.
---------------------
When considering cases of n=25,200,and 1000, N=3, where the items are in two sets at first, then merged into one with a tag perturbation,
we find that the speedup of this new algorithm generally tends to increase as n increases, however in the diagonal case,
the speedup tends to decrease as n increases. In other words, the algorithm behaves like a polynomial-time
algorithm while the size of the descriptor is small
---------------------
poly_add algorithm 5.272x on 10diagonal at n=12000
poly_add_mem algorithm 9.691x on 10diagonal at n=12000
poly_add algorithm 0.7007x on 1000diagonal at n=12
poly_add_mem algorithm 1.052x on 1000diagonal at n=12

Conclusion: memory seems to be an issue. Future iterations of this algorithm will take that into account.
---------------------
Max Flow Formulation:
Turned out to be SLOWER than the original poly-time solution (makes sense).
On 100diagonal, n=12, the max flow formulation was found to be 0.0874x the speed of the ILP.
After further experimentation, it ran slower than the ILP solution by a large margin, even when not recalculating the
max flow on every iteration. For some reason, the speedup was only 1.7489x when for loop was completely ignored.
That is, when the answer was not calculated, there was only a 1.7489x speedup.
Also of note, consolidating for loops in the beginning brought this to a 2.086x speedup.